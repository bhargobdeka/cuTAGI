{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8j/ybqbv42143jfglvsf83t5s_c0000gn/T/ipykernel_93003/3833975703.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/bhargobdeka/Desktop/cuTAGI/python_examples\n"
     ]
    }
   ],
   "source": [
    "pwd = os.getcwd()\n",
    "print(pwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/bhargobdeka/Desktop/cuTAGI\n"
     ]
    }
   ],
   "source": [
    "# go one step out of the current directory\n",
    "os.chdir('..')\n",
    "pwd = os.getcwd()\n",
    "print(pwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_examples.data_loader import RegressionDataLoader\n",
    "from python_examples.regression import Regression\n",
    "from pytagi import NetProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_file_path = '/Users/bhargobdeka/Desktop/cuTAGI/data/UCI/Boston_housing/data/data.txt'\n",
    "\n",
    "data_name = 'data/UCI/Boston_housing'\n",
    "\n",
    "data = np.loadtxt(data_name + '/data/data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 14)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load the indexes for the features and for the target\n",
    "\n",
    "index_features = np.loadtxt(data_name +'/data/index_features.txt').astype(int)\n",
    "index_target   = np.loadtxt(data_name +'/data/index_target.txt').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## User-defined parameters\n",
    "# splits\n",
    "n_splits  = 1\n",
    "\n",
    "# User-input\n",
    "num_inputs = 13     # 1 explanatory variable\n",
    "num_outputs = 1     # 1 predicted output\n",
    "num_epochs = 50     # row for 50 epochs\n",
    "BATCH_SIZE = 1      # batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## classes\n",
    "class HeterosUCIMLP(NetProp):\n",
    "    \"\"\"Multi-layer preceptron for regression task where the\n",
    "    output's noise varies overtime\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.layers =       [1, 1, 1]\n",
    "        self.nodes =        [13, 50, 2]  # output layer = [mean, std]\n",
    "        self.activations =  [0, 4, 0]\n",
    "        self.batch_size =   BATCH_SIZE\n",
    "        self.sigma_v =      0\n",
    "        self.sigma_v_min =  0\n",
    "        self.noise_gain =   1\n",
    "        self.noise_type =   \"heteros\"\n",
    "        self.init_method =  \"He\"\n",
    "        self.device =       \"cpu\"\n",
    "\n",
    "## Functions\n",
    "def create_data_loader(raw_input: np.ndarray, raw_output: np.ndarray, batch_size) -> list:\n",
    "        \"\"\"Create dataloader based on batch size\"\"\"\n",
    "        num_input_data = raw_input.shape[0]\n",
    "        num_output_data = raw_output.shape[0]\n",
    "        assert num_input_data == num_output_data\n",
    "\n",
    "        # Even indices\n",
    "        even_indices = split_evenly(num_input_data, batch_size)\n",
    "\n",
    "        if np.mod(num_input_data, batch_size) != 0:\n",
    "            # Remider indices\n",
    "            rem_indices = split_reminder(num_input_data, batch_size)\n",
    "            even_indices.append(rem_indices)\n",
    "\n",
    "        indices = np.stack(even_indices)\n",
    "        input_data = raw_input[indices]\n",
    "        output_data = raw_output[indices]\n",
    "        dataset = []\n",
    "        for x_batch, y_batch in zip(input_data, output_data):\n",
    "            dataset.append((x_batch, y_batch))\n",
    "        return dataset\n",
    "\n",
    "\n",
    "def split_evenly(num_data, chunk_size: int):\n",
    "    \"\"\"split data evenly\"\"\"\n",
    "    indices = np.arange(int(num_data - np.mod(num_data, chunk_size)))\n",
    "\n",
    "    return np.split(indices, int(np.floor(num_data / chunk_size)))\n",
    "\n",
    "def split_reminder(num_data: int, chunk_size: int):\n",
    "        \"\"\"Pad the reminder\"\"\"\n",
    "        indices = np.arange(num_data)\n",
    "        reminder_start = int(num_data - np.mod(num_data, chunk_size))\n",
    "        num_samples = chunk_size - (num_data - reminder_start)\n",
    "        random_idx = np.random.choice(indices, size=num_samples, replace=False)\n",
    "        reminder_idx = indices[reminder_start:]\n",
    "\n",
    "        return np.concatenate((random_idx, reminder_idx))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data and output data\n",
    "X = data[ : , index_features.tolist() ]\n",
    "Y = data[ : , index_target.tolist() ]\n",
    "input_dim = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((506, 13), (506,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the specific train and test indices\n",
    "index_train = np.loadtxt(data_name +\"/data/index_train_{}.txt\".format(0)).astype(int)\n",
    "index_test = np.loadtxt(data_name +\"/data/index_test_{}.txt\".format(0)).astype(int)\n",
    "\n",
    "# Train and Test data for the current split\n",
    "x_train = X[ index_train.tolist(), ]\n",
    "y_train = Y[ index_train.tolist() ]\n",
    "y_train = np.reshape(y_train,[len(y_train),1]) #BD\n",
    "x_test  = X[ index_test.tolist(), ]\n",
    "y_test  = Y[ index_test.tolist() ]\n",
    "y_test = np.reshape(y_test,[len(y_test),1])    #BD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((455, 13), (455, 1), (51, 13), (51, 1))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first few rows of the training or test data\n",
    "print(list(x_test[:5,]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytagi import Normalizer, Utils\n",
    "\n",
    "# Normalizer\n",
    "normalizer: Normalizer = Normalizer()\n",
    "\n",
    "x_mean, x_std = normalizer.compute_mean_std(x_train)\n",
    "y_mean, y_std = normalizer.compute_mean_std(y_train)\n",
    "\n",
    "# x_mean, x_std = normalizer.compute_mean_std(\n",
    "#     np.concatenate((x_train, x_test))\n",
    "# )\n",
    "# y_mean, y_std = normalizer.compute_mean_std(\n",
    "#     np.concatenate((y_train, y_test))\n",
    "# )\n",
    "\n",
    "# Normalizing the data\n",
    "x_train = normalizer.standardize(data=x_train, mu=x_mean, std=x_std)\n",
    "y_train = normalizer.standardize(data=y_train, mu=y_mean, std=y_std)\n",
    "x_test = normalizer.standardize(data=x_test, mu=x_mean, std=x_std)\n",
    "y_test = normalizer.standardize(data=y_test, mu=y_mean, std=y_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22.77846154] [9.32785371]\n"
     ]
    }
   ],
   "source": [
    "# print y_mean, y_std\n",
    "print(y_mean, y_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "data_loader = {}\n",
    "data_loader[\"train\"] = (x_train, y_train)\n",
    "data_loader[\"test\"] = create_data_loader(\n",
    "    raw_input=x_test, raw_output=y_test, batch_size=BATCH_SIZE\n",
    ")\n",
    "data_loader[\"x_norm_param_1\"] = x_mean\n",
    "data_loader[\"x_norm_param_2\"] = x_std\n",
    "data_loader[\"y_norm_param_1\"] = y_mean\n",
    "data_loader[\"y_norm_param_2\"] = y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "net_prop = HeterosUCIMLP()\n",
    "\n",
    "# Regression loader\n",
    "reg_data_loader = RegressionDataLoader(num_inputs=num_inputs,\n",
    "                                    num_outputs=num_outputs,\n",
    "                                    batch_size=net_prop.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression task\n",
    "reg_task = Regression(num_epochs=num_epochs,\n",
    "                    data_loader=data_loader,\n",
    "                    net_prop=net_prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "reg_task.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "reg_task.predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tagi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
